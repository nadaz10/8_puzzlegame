{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadaz10/8_puzzlegame/blob/main/NLP_assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting dataset"
      ],
      "metadata": {
        "id": "zhv37jNcClyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne9toyr5CkLp",
        "outputId": "c54b6f30-f39e-4c0a-f092-df6d6777b197"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset"
      ],
      "metadata": {
        "id": "z3D5D_RoDAhW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sst_dataset = load_dataset('sst')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NdKuRUsDJw7",
        "outputId": "7c93cacb-46ee-4815-cd41-ba346b15792d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for sst contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/sst\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sst_dataset['train'][500])\n",
        "print(f\"Number of training examples: {len(sst_dataset['train'])}\")\n",
        "print(f\"Number of validation examples: {len(sst_dataset['validation'])}\")\n",
        "print(f\"Number of test examples: {len(sst_dataset['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xAHI1AxDKqm",
        "outputId": "c820e89b-8a04-4662-f16a-39e59d19096f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': 'This story still seems timely and important .', 'label': 0.6944400072097778, 'tokens': 'This|story|still|seems|timely|and|important|.', 'tree': '14|14|13|11|9|9|10|12|10|11|12|13|15|15|0'}\n",
            "Number of training examples: 8544\n",
            "Number of validation examples: 1101\n",
            "Number of test examples: 2210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "9IHBlanSB3eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the 'tree' column and adjusting the labels to accomodate the 5 classes we want to classify to where:\n",
        "From 0 to 0.2 (0.2 included) will be class 0 “very negative”.\n",
        "\n",
        "From 0.2 to 0.4 (0.4 included) will be class 1 “negative”.\n",
        "\n",
        "From 0.4 to 0.6 (0.6 included) will be class 2 “neutral”.\n",
        "\n",
        "From 0.6 to 0.8 (0.8 included) will be class 3 “positive”.\n",
        "\n",
        "From 0.8 to 1.0 (1.0 included) will be class 4 “very positive”."
      ],
      "metadata": {
        "id": "a6rfAZ8_TfEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting splits to Pandas DataFrame for manipulation\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(sst_dataset['train'])\n",
        "test_df = pd.DataFrame(sst_dataset['test'])\n",
        "val_df = pd.DataFrame(sst_dataset['validation'])\n",
        "\n",
        "#function to map float labels to our desired categorical classes\n",
        "def map_labels_to_categories(label):\n",
        "    if 0.0 <= label <= 0.2:\n",
        "        return 0  # very negative\n",
        "    elif 0.2 < label <= 0.4:\n",
        "        return 1  # negative\n",
        "    elif 0.4 < label <= 0.6:\n",
        "        return 2  # neutral\n",
        "    elif 0.6 < label <= 0.8:\n",
        "        return 3  # positive\n",
        "    elif 0.8 < label <= 1.0:\n",
        "        return 4  # very positive\n",
        "\n",
        "#mapping function to create a new column with mapped labels\n",
        "train_df['mapped_label'] = train_df['label'].apply(map_labels_to_categories)\n",
        "test_df['mapped_label'] = test_df['label'].apply(map_labels_to_categories)\n",
        "val_df['mapped_label'] = val_df['label'].apply(map_labels_to_categories)\n",
        "\n",
        "#dropping the original label column\n",
        "train_df = train_df.drop('label', axis=1)\n",
        "test_df = test_df.drop('label', axis=1)\n",
        "val_df = val_df.drop('label', axis=1)\n",
        "\n",
        "train_df = train_df.drop(columns=['tree']) #dropping 'tree' column\n",
        "test_df = test_df.drop(columns=['tree'])\n",
        "val_df = val_df.drop(columns=['tree'])\n",
        "\n",
        "\n",
        "#convert the DataFrame back to the datasets format\n",
        "sst_dataset['train'] = Dataset.from_pandas(train_df)\n",
        "sst_dataset['test'] = Dataset.from_pandas(test_df)\n",
        "sst_dataset['validation'] = Dataset.from_pandas(val_df)\n",
        "\n",
        "#display the updated dataset\n",
        "print(sst_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5EPCqgimpFE",
        "outputId": "6d66ef0d-9e3f-4588-8081-028c80aa1287"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'tokens', 'mapped_label'],\n",
            "        num_rows: 8544\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence', 'tokens', 'mapped_label'],\n",
            "        num_rows: 1101\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'tokens', 'mapped_label'],\n",
            "        num_rows: 2210\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#analyzing the distribution of labels\n",
        "train_label_distribution = train_df['mapped_label'].value_counts().sort_index()\n",
        "test_label_distribution = test_df['mapped_label'].value_counts().sort_index()\n",
        "val_label_distribution = val_df['mapped_label'].value_counts().sort_index()\n",
        "\n",
        "print(\"Training Split Label Distribution:\")\n",
        "print(train_label_distribution)\n",
        "\n",
        "print(\"\\nTest Split Label Distribution:\")\n",
        "print(test_label_distribution)\n",
        "\n",
        "print(\"\\nValidation Split Label Distribution:\")\n",
        "print(val_label_distribution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "688DssCrsigk",
        "outputId": "e59e8ac6-0de2-4bad-d050-d07b1f684574"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Split Label Distribution:\n",
            "0    1092\n",
            "1    2218\n",
            "2    1624\n",
            "3    2322\n",
            "4    1288\n",
            "Name: mapped_label, dtype: int64\n",
            "\n",
            "Test Split Label Distribution:\n",
            "0    279\n",
            "1    633\n",
            "2    389\n",
            "3    510\n",
            "4    399\n",
            "Name: mapped_label, dtype: int64\n",
            "\n",
            "Validation Split Label Distribution:\n",
            "0    139\n",
            "1    289\n",
            "2    229\n",
            "3    279\n",
            "4    165\n",
            "Name: mapped_label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualisation for us"
      ],
      "metadata": {
        "id": "GbDu1oUutxsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(sst_dataset['train']) #convert the 'train' split to a Pandas DataFrame\n",
        "\n",
        "# Plot the distribution of mapped labels in the 'train' split\n",
        "train_df['mapped_label'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Distribution of Mapped Labels in the Train Split')\n",
        "plt.xlabel('Mapped Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "5ZmT1vVBd1Fy",
        "outputId": "3afb3355-3e18-4159-b9d5-3d36c2bfcc67"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few rows of the 'train' split to see the transformed data\n",
        "print(train_df.head())\n",
        "\n",
        "# Check the unique values in the 'mapped_label' column of the 'train' split to ensure correct mapping\n",
        "print(train_df['mapped_label'].unique())\n",
        "\n",
        "# Check for missing values in the 'mapped_label' column of the 'train' split\n",
        "print(train_df['mapped_label'].isnull().sum())\n",
        "# # Print the first few rows of the 'train' split to see the transformed data\n",
        "# print(sst_dataset['train'].head())\n",
        "\n",
        "# # Check the unique values in the 'mapped_label' column of the 'train' split to ensure correct mapping\n",
        "# print(sst_dataset['train']['mapped_label'].unique())\n",
        "\n",
        "# # Check for missing values in the 'mapped_label' column of the 'train' split\n",
        "# print(sst_dataset['train']['mapped_label'].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_iFiVO7TAe6",
        "outputId": "8ef91085-9cb2-4a6a-812a-7a19f737179f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            sentence  \\\n",
            "0  The Rock is destined to be the 21st Century 's...   \n",
            "1  The gorgeously elaborate continuation of `` Th...   \n",
            "2  Singer\\/composer Bryan Adams contributes a sle...   \n",
            "3  You 'd think by now America would have had eno...   \n",
            "4               Yet the act is still charming here .   \n",
            "\n",
            "                                              tokens  mapped_label  \n",
            "0  The|Rock|is|destined|to|be|the|21st|Century|'s...             3  \n",
            "1  The|gorgeously|elaborate|continuation|of|``|Th...             4  \n",
            "2  Singer\\/composer|Bryan|Adams|contributes|a|sle...             3  \n",
            "3  You|'d|think|by|now|America|would|have|had|eno...             2  \n",
            "4               Yet|the|act|is|still|charming|here|.             3  \n",
            "[3 4 2 1 0]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes\n"
      ],
      "metadata": {
        "id": "Ngi3yEz6q7rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # Tokenize the text\n",
        "    tokens = [doc.split() for doc in data]\n",
        "\n",
        "    # Create a vocabulary\n",
        "    vocab = set(word for doc in tokens for word in doc)\n",
        "\n",
        "    # Convert text to bag-of-words representation\n",
        "    bow_representation = []\n",
        "    for doc in tokens:\n",
        "        bow = np.zeros(len(vocab))\n",
        "        for word in doc:\n",
        "            if word in vocab:\n",
        "                bow[vocab.index(word)] += 1\n",
        "        bow_representation.append(bow)\n",
        "\n",
        "    return bow_representation, vocab"
      ],
      "metadata": {
        "id": "yfBtEIQPq1bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sf6IBPCXvRuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_naive_bayes(data, labels, num_classes):\n",
        "    # Calculate the number of documents and the vocabulary size\n",
        "    num_docs = len(data)\n",
        "    vocab = set(word for doc in data for word in doc)\n",
        "\n",
        "    # Initialize the log prior and log likelihood matrices\n",
        "    log_prior = np.zeros(num_classes)\n",
        "    log_likelihood = np.zeros((len(vocab), num_classes))\n",
        "\n",
        "    # Calculate the log prior probabilities\n",
        "    for c in range(num_classes):\n",
        "        num_c = sum(1 for label in labels if label == c)\n",
        "        log_prior[c] = np.log(num_c / num_docs)\n",
        "\n",
        "    # Calculate the log likelihood probabilities\n",
        "    for c in range(num_classes):\n",
        "        class_docs = [doc for doc, label in zip(data, labels) if label == c]\n",
        "        for i, word in enumerate(vocab):\n",
        "            count = sum(doc[i] for doc in class_docs if i < len(doc))\n",
        "            log_likelihood[i, c] = np.log((count + 1) / (sum(doc[i] for doc in class_docs) + len(vocab)))\n",
        "\n",
        "    return log_prior, log_likelihood, vocab"
      ],
      "metadata": {
        "id": "F3gJUJX6rGBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_naive_bayes(test_doc, log_prior, log_likelihood, num_classes, vocab):\n",
        "    # Convert the test document to a bag-of-words representation\n",
        "    test_bow = np.zeros(len(vocab))\n",
        "    for i, word in enumerate(test_doc):\n",
        "        if word in vocab:\n",
        "            test_bow[vocab.index(word)] += 1\n",
        "\n",
        "    # Calculate the log probability for each class\n",
        "    log_prob = np.zeros(num_classes)\n",
        "    for c in range(num_classes):\n",
        "        log_prob[c] = log_prior[c] + np.sum(test_bow * log_likelihood[:, c])\n",
        "\n",
        "    # Return the class with the highest log probability\n",
        "    return np.argmax(log_prob)"
      ],
      "metadata": {
        "id": "DlXfyPLfrGId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "XhtyVNHsJ3zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "fqg5RdJfPoWm"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def features_extract(data):\n",
        "    # Initialize a set to store sentence and bigrams pairs\n",
        "    sentence_bigrams_set = set()\n",
        "    unique_bigrams = set()\n",
        "\n",
        "    # Extract relevant information from the dataset\n",
        "    for i in range(len(data)+1):\n",
        "        # Initialize a list to store unique word bi-grams\n",
        "        sentence = data['sentence'][i]\n",
        "        tokens = data['tokens'][i].split('|')\n",
        "\n",
        "        # Build bigrams\n",
        "        bigrams = [f\"{tokens[j]} {tokens[j + 1]}\" for j in range(len(tokens) -1)]\n",
        "\n",
        "        # Add the pair to the set\n",
        "        sentence_bigrams_set.add((sentence, tuple(bigrams)))\n",
        "        unique_bigrams.update(set(bigrams))\n",
        "\n",
        "    # Create a list of unique words\n",
        "    unique_words = sorted(list(set(word for sentence in data['tokens'] for word in sentence.split('|'))))\n",
        "    print(unique_bigrams)\n",
        "    # Create a binary matrix to store bigram existence\n",
        "    bigram_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)\n",
        "\n",
        "    # Populate the matrix\n",
        "    for i, word1 in enumerate(unique_words):\n",
        "        for j, word2 in enumerate(unique_words):\n",
        "            print(word1, word2)\n",
        "            current_bigram = f\"{word1} {word2}\"\n",
        "            if current_bigram in unique_bigrams:\n",
        "                print(\"!!!\")\n",
        "                bigram_matrix[i, j] = 1\n",
        "\n",
        "    return bigram_matrix, unique_words\n",
        "\n",
        "# Example usage with a dummy dataset\n",
        "sst_dataset = {\n",
        "    'tokens': [\"apple|orange|banana\", \"orange|grape|kiwi\", \"kiwi|banana|apple\"],\n",
        "    'sentence': [\"This is the first sentence.\", \"Another sentence here.\", \"And one more sentence.\"]\n",
        "}\n",
        "\n",
        "train_dataset = sst_dataset  # Replace with your actual dataset\n",
        "binary_bigram_matrix, unique_words = features_extract(train_dataset)\n",
        "\n",
        "# Print the resulting matrix\n",
        "print(\"Binary Bigram Matrix:\")\n",
        "print(binary_bigram_matrix)\n"
      ],
      "metadata": {
        "id": "YH67nqJLgl_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def features_extract(data):\n",
        "    # Initialize a set to store sentence and bigrams pairs\n",
        "    sentence_bigrams_set = set()\n",
        "    unique_bigrams = set()\n",
        "    # Extract relevant information from the dataset\n",
        "    for i in range(len(data)):\n",
        "        # Initialize a list to store unique word bi-grams\n",
        "        sentence = data['sentence'][i]\n",
        "        tokens = data['tokens'][i].split('|')\n",
        "\n",
        "        # Build bigrams\n",
        "        bigrams = [f\"{tokens[j]} {tokens[j + 1]}\" for j in range(len(tokens) - 1)]\n",
        "        # print(bigrams)\n",
        "        # Add the pair to the set\n",
        "\n",
        "        sentence_bigrams_set.add((sentence, tuple(bigrams)))\n",
        "        unique_bigrams.update(set(bigrams))\n",
        "\n",
        "        # Create a list of unique words\n",
        "    unique_words = sorted(list(set(word for sentence in data['tokens'] for word in sentence.split('|'))))\n",
        "\n",
        "        # Create a binary matrix to store bigram existence\n",
        "    bigram_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)\n",
        "\n",
        "        # Populate the matrix\n",
        "    for i, word1 in enumerate(unique_words):\n",
        "            for j, word2 in enumerate(unique_words):\n",
        "              current_bigram = f\"{word1} {word2}\"\n",
        "              if current_bigram in unique_bigrams:\n",
        "                  print(\"!!!\")\n",
        "                  bigram_matrix[i, j] = 1\n",
        "    return bigram_matrix, unique_words\n",
        "\n",
        "\n",
        "\n",
        "# Assuming you have a dataset named 'sst_dataset'\n",
        "train_dataset = sst_dataset['train']\n",
        "sentence_bigrams_set, unique_words = features_extract(train_dataset)\n",
        "\n",
        "# # Print the resulting set\n",
        "# for sentence, bigrams in sentence_bigrams_set:\n",
        "#     print(\"Sentence:\", sentence)\n",
        "#     print(\"Bigrams:\", bigrams)\n",
        "#     print()\n",
        "\n"
      ],
      "metadata": {
        "id": "xxhS6wtyXArv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the sum of the matrix\n",
        "matrix_sum = np.sum(sentence_bigrams_set)\n",
        "print(\"Sum of the matrix:\", matrix_sum)\n",
        "plt.imshow(sentence_bigrams_set, cmap='binary', interpolation='nearest')\n",
        "plt.title('Binary Bigram Matrix Visualization')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xE-VEmCRH_8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MultiClassLogisticRegression:\n",
        "\n",
        "    def __init__(self, n_iter = 10000, thres=1e-3):\n",
        "        self.n_iter = n_iter\n",
        "        self.thres = thres\n",
        "        self.classes = ['0','1','2','3','4']\n",
        "        self.class_labels = ['0','1','2','3','4']\n",
        "\n",
        "    def fit(self, X, y, batch_size=64, lr=0.001, rand_seed=4, verbose=False):\n",
        "        np.random.seed(rand_seed)\n",
        "        self.classes = np.unique(y)\n",
        "        self.class_labels = {c:i for i,c in enumerate(self.classes)}\n",
        "        X = self.add_bias(X)\n",
        "        y = self.one_hot(y)\n",
        "        self.loss = []\n",
        "        self.weights = np.zeros(shape=(len(self.classes),X.shape[1]))\n",
        "        self.fit_data(X, y, batch_size, lr, verbose)\n",
        "        return self\n",
        "\n",
        "    def fit_data(self, X, y, batch_size, lr, verbose):\n",
        "        i = 0\n",
        "        while (not self.n_iter or i < self.n_iter):\n",
        "            self.loss.append(self.cross_entropy(y, self.predict_(X)))\n",
        "            idx = np.random.choice(X.shape[0], batch_size)\n",
        "            X_batch, y_batch = X[idx], y[idx]\n",
        "            error = y_batch - self.predict_(X_batch)\n",
        "            update = (lr * np.dot(error.T, X_batch))\n",
        "            self.weights += update\n",
        "            if np.abs(update).max() < self.thres: break\n",
        "            if i % 1000 == 0 and verbose:\n",
        "                print(' Training Accuray at {} iterations is {}'.format(i, self.evaluate_(X, y)))\n",
        "            i +=1\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.predict_(self.add_bias(X))\n",
        "\n",
        "    def predict_(self, X):\n",
        "        pre_vals = np.dot(X, self.weights.T).reshape(-1,len(self.classes))\n",
        "        return self.softmax(pre_vals)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1,1)\n",
        "\n",
        "    def predict_classes(self, X):\n",
        "        self.probs_ = self.predict(X)\n",
        "        return np.vectorize(lambda c: self.classes[c])(np.argmax(self.probs_, axis=1))\n",
        "\n",
        "    def add_bias(self,X):\n",
        "        return np.insert(X, 0, 1, axis=1)\n",
        "\n",
        "    def get_randon_weights(self, row, col):\n",
        "        return np.zeros(shape=(row,col))\n",
        "\n",
        "    def one_hot(self, y):\n",
        "        return np.eye(len(self.classes))[np.vectorize(lambda c: self.class_labels[c])(y).reshape(-1)]\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return np.mean(self.predict_classes(X) == y)\n",
        "\n",
        "    def evaluate_(self, X, y):\n",
        "        return np.mean(np.argmax(self.predict_(X), axis=1) == np.argmax(y, axis=1))\n",
        "\n",
        "    def cross_entropy(self, y, probs):\n",
        "        return -1 * np.mean(y * np.log(probs))\n",
        "\n",
        "\n",
        "\n",
        "multireg = MultiClassLogisticRegression()\n",
        "y_hot = multireg.one_hot(train_dataset['mapped_label'])\n",
        "multireg.fit(sentence_bigrams_set,y_hot)\n",
        "print(multireg.score(sentence_bigrams_set,y_hot))"
      ],
      "metadata": {
        "id": "sLsr65wXrxvu",
        "outputId": "d4a56e08-b200-477c-e506-15de7534876b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "arrays used as indices must be of integer (or boolean) type",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-b7922e9f4915>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mmultireg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiClassLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0my_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultireg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mapped_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mmultireg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_bigrams_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultireg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_bigrams_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-b7922e9f4915>\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(Z):\n",
        "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "\n",
        "def one_hot(y):\n",
        "        return np.eye(len(self.classes))[np.vectorize(lambda c: self.class_labels[c])(y).reshape(-1)]\n",
        "\n",
        "def initialize_parameters(num_features, num_classes):\n",
        "    W = np.random.randn(num_features, num_classes) * 0.01\n",
        "    b = np.zeros((1, num_classes))\n",
        "    return W, b\n",
        "\n",
        "def forward_propagation(X, W, b):\n",
        "    Z = np.dot(X, W) + b\n",
        "    A = softmax(Z)\n",
        "    return A\n",
        "\n",
        "def compute_loss(A, Y):\n",
        "    m = Y.shape[0]\n",
        "    loss = -np.sum(Y * np.log(A + 1e-8)) / m  # Add a small constant to avoid log(0)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def backward_propagation(X, A, Y):\n",
        "    m = X.shape[0]\n",
        "    dZ = A - Y\n",
        "    dW = np.dot(X.T, dZ) / m\n",
        "    db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "    return dW, db\n",
        "\n",
        "def update_parameters(W, b, dW, db, learning_rate):\n",
        "    W -= learning_rate * dW\n",
        "    b -= learning_rate * db\n",
        "    return W, b\n",
        "\n",
        "def train_multiclass_logistic_regression(X, Y, num_classes, learning_rate, num_epochs):\n",
        "        np.random.seed(rand_seed)\n",
        "        self.classes = np.unique(y)\n",
        "        self.class_labels = {c:i for i,c in enumerate(self.classes)}\n",
        "        X = self.add_bias(X)\n",
        "        y = self.one_hot(y)\n",
        "        self.loss = []\n",
        "        self.weights = np.zeros(shape=(len(self.classes),X.shape[1]))\n",
        "        self.fit_data(X, y, batch_size, lr, verbose)\n",
        "        return self\n",
        "\n",
        "    def fit_data(self, X, y, batch_size, lr, verbose):\n",
        "        i = 0\n",
        "        while (not self.n_iter or i < self.n_iter):\n",
        "            self.loss.append(self.cross_entropy(y, self.predict_(X)))\n",
        "            idx = np.random.choice(X.shape[0], batch_size)\n",
        "            X_batch, y_batch = X[idx], y[idx]\n",
        "            error = y_batch - self.predict_(X_batch)\n",
        "            update = (lr * np.dot(error.T, X_batch))\n",
        "            self.weights += update\n",
        "            if np.abs(update).max() < self.thres: break\n",
        "            if i % 1000 == 0 and verbose:\n",
        "                print(' Training Accuray at {} iterations is {}'.format(i, self.evaluate_(X, y)))\n",
        "            i +=1\n",
        "\n",
        "\n",
        "\n",
        "# One-hot encode the labels\n",
        "Y_train = one_hot(train_dataset['mapped_label'], num_classes=5)\n",
        "\n",
        "# Training\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "num_classes = 3\n",
        "\n",
        "trained_W, trained_b = train_multiclass_logistic_regression(sentence_bigrams_set, Y_train, num_classes, learning_rate, num_epochs)\n"
      ],
      "metadata": {
        "id": "yOft7Ap-Khrr",
        "outputId": "458b5133-9c96-4fc3-e1a0-a3c6db2d9367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (8544,) (8544,5) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-d0562b826106>\u001b[0m in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mtrained_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_multiclass_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_bigrams_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-77-d0562b826106>\u001b[0m in \u001b[0;36mtrain_multiclass_logistic_regression\u001b[0;34m(X, Y, num_classes, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Compute one-hot encoded labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mencoded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-d0562b826106>\u001b[0m in \u001b[0;36mone_hot_encode\u001b[0;34m(labels, num_classes)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mencoded_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (8544,) (8544,5) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have a binary bigram matrix\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "# Print the entire binary bigram matrix\n",
        "print(\"Binary Bigram Matrix:\")\n",
        "print(sentence_bigrams_set)"
      ],
      "metadata": {
        "id": "eTRHiR4lDcGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def features_extract(data):\n",
        "    # Initialize a set to store unique word bi-grams\n",
        "    unique_bi_grams = set()\n",
        "\n",
        "    # Extract relevant information from the dataset\n",
        "    for i in range(len(data)):\n",
        "        tokens = data['tokens'][i].split('|')\n",
        "\n",
        "        # Build unique bigrams\n",
        "        bigrams = {f\"{tokens[j]} {tokens[j + 1]}\" for j in range(len(tokens) - 1)}\n",
        "\n",
        "        # Update the set of unique bi-grams\n",
        "        unique_bi_grams.update(bigrams)\n",
        "\n",
        "    # Create a vocabulary mapping bi-grams to indices\n",
        "    vocabulary = {bi_gram: i for i, bi_gram in enumerate(unique_bi_grams)}\n",
        "\n",
        "    # Initialize a sparse matrix to store the features\n",
        "    num_samples = len(data)\n",
        "    num_features = len(vocabulary)\n",
        "    features = lil_matrix((num_samples, num_features), dtype=np.int8)\n",
        "\n",
        "    # Fill the sparse matrix with 1s where the bi-gram exists in each sentence\n",
        "    for i in range(len(data)):\n",
        "        tokens = data['tokens'][i].split('|')\n",
        "        sentence_bigrams = {f\"{tokens[j]} {tokens[j + 1]}\" for j in range(len(tokens) - 1)}\n",
        "\n",
        "        for bi_gram in sentence_bigrams:\n",
        "            if bi_gram in vocabulary:\n",
        "                features[i, vocabulary[bi_gram]] = 1\n",
        "\n",
        "    return features, vocabulary\n",
        "\n",
        "# Assuming you have a dataset named 'sst_dataset'\n",
        "train_dataset = sst_dataset['train']\n",
        "X_train_sparse_features, vocabulary = features_extract(train_dataset)"
      ],
      "metadata": {
        "id": "X9Xfihl7q94Y"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_sparse_features)"
      ],
      "metadata": {
        "id": "SX3IWA-wotWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_XDsuV2CbQV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}